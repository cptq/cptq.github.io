<!DOCTYPE html>
<html lang="en-us">

<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta http-equiv="X-UA-Compatible" content="ie=edge">
	<meta name="theme-color" content="#FFF2FC">
	<meta name="msapplication-TileColor" content="#FFF2FC">
<meta itemprop="name" content="Universal Invariant Networks Through a Nice Commutative Diagram">
<meta itemprop="description" content="\(\newcommand{\RR}{\mathbb{R}}\) \(\newcommand{\NN}{\mathbb{N}}\)
\(\newcommand{\mc}{\mathcal}\)
When working on a project with group invariant neural networks, I found myself repeatedly using essentially the same steps to prove many different results on universality of invariant networks. I eventually realized that these steps could be captured in the following commutative diagram:
  What’s more: this commutative diagram helps simplify and unify previous proofs of results in the study of invariant neural networks. Plus, it gives a blueprint for developing novel invariant neural network architectures."><meta itemprop="datePublished" content="2022-03-06T00:00:00&#43;00:00" />
<meta itemprop="dateModified" content="2022-03-06T00:00:00&#43;00:00" />
<meta itemprop="wordCount" content="2723">
<meta itemprop="keywords" content="" /><meta property="og:title" content="Universal Invariant Networks Through a Nice Commutative Diagram" />
<meta property="og:description" content="\(\newcommand{\RR}{\mathbb{R}}\) \(\newcommand{\NN}{\mathbb{N}}\)
\(\newcommand{\mc}{\mathcal}\)
When working on a project with group invariant neural networks, I found myself repeatedly using essentially the same steps to prove many different results on universality of invariant networks. I eventually realized that these steps could be captured in the following commutative diagram:
  What’s more: this commutative diagram helps simplify and unify previous proofs of results in the study of invariant neural networks. Plus, it gives a blueprint for developing novel invariant neural network architectures." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cptq.github.io/posts/universal_invariant/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-03-06T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2022-03-06T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Universal Invariant Networks Through a Nice Commutative Diagram"/>
<meta name="twitter:description" content="\(\newcommand{\RR}{\mathbb{R}}\) \(\newcommand{\NN}{\mathbb{N}}\)
\(\newcommand{\mc}{\mathcal}\)
When working on a project with group invariant neural networks, I found myself repeatedly using essentially the same steps to prove many different results on universality of invariant networks. I eventually realized that these steps could be captured in the following commutative diagram:
  What’s more: this commutative diagram helps simplify and unify previous proofs of results in the study of invariant neural networks. Plus, it gives a blueprint for developing novel invariant neural network architectures."/>

	<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
	<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
	<link rel="manifest" href="/site.webmanifest">
	<link rel="mask-icon" href="/safari-pinned-tab.svg" color="">
	<link rel="shortcut icon" href="/favicon.ico">

	<title>Universal Invariant Networks Through a Nice Commutative Diagram</title>
	<link rel="stylesheet" href="https://cptq.github.io/css/style.min.d3dfda823ff1cc9b72efb37de41f475faccde98c8ead2f64e571e72700608547.css" integrity="sha256-09/agj/xzJty77N95B9HX6zN6YyOrS9k5XHnJwBghUc=" crossorigin="anonymous">
	
</head>

<body id="page">
	
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
	<header id="site-header" class="animated slideInUp faster">
		<div class="hdr-wrapper section-inner">
			<div class="hdr-left">
				<div class="site-branding">
					<a href="https://cptq.github.io">Derek Lim</a>
				</div>
				<nav class="site-nav hide-in-mobile">
					
				<a href="https://cptq.github.io/about/">About</a>
				<a href="https://cptq.github.io/papers/">Papers</a>
				<a href="https://cptq.github.io/posts/">Posts</a>
				<a href="https://cptq.github.io/other/">Other</a>

				</nav>
			</div>
			<div class="hdr-right hdr-icons">
				<span class="hdr-social hide-in-mobile"><a href="/contact/" target="_blank" rel="noopener me" title="Email"><svg xmlns="http://www.w3.org/2000/svg" class="feather" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z"></path><polyline points="22,6 12,13 2,6"></polyline></svg></a><a href="https://github.com/cptq" target="_blank" rel="noopener me" title="Github"><svg xmlns="http://www.w3.org/2000/svg" class="feather" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path></svg></a><a href="https://www.linkedin.com/in/lim-derek/" target="_blank" rel="noopener me" title="Linkedin"><svg xmlns="http://www.w3.org/2000/svg" class="feather" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path><rect x="2" y="9" width="4" height="12"></rect><circle cx="4" cy="4" r="2"></circle></svg></a><a href="https://twitter.com/dereklim_lzh" target="_blank" rel="noopener me" title="Twitter"><svg xmlns="http://www.w3.org/2000/svg" class="feather" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M23 3a10.9 10.9 0 0 1-3.14 1.53 4.48 4.48 0 0 0-7.86 3v1A10.66 10.66 0 0 1 3 4s-4 9 5 13a11.64 11.64 0 0 1-7 2c9 5 20 0 20-11.5a4.5 4.5 0 0 0-.08-.83A7.72 7.72 0 0 0 23 3z"></path></svg></a><a href="https://scholar.google.com/citations?user=y9YTBIsAAAAJ&amp;hl=en" target="_blank" rel="noopener me" title="Scholar"><img src="/scholar.png" width="24" height="24"> </img></a></span><button id="menu-btn" class="hdr-btn" title="Menu"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line></svg></button>
			</div>
		</div>
	</header>
	<div id="mobile-menu" class="animated fast">
		<ul>
			<li><a href="https://cptq.github.io/about/">About</a></li>
			<li><a href="https://cptq.github.io/papers/">Papers</a></li>
			<li><a href="https://cptq.github.io/posts/">Posts</a></li>
			<li><a href="https://cptq.github.io/other/">Other</a></li>
		</ul>
	</div>


	<main class="site-main section-inner animated fadeIn faster">
		<article class="thin">
			<header class="post-header">
				<div class="post-meta"><span>Mar 6, 2022</span></div>
				<h1>Universal Invariant Networks Through a Nice Commutative Diagram</h1>
			</header>
			<div class="content">
				<p><span class="math inline">\(\newcommand{\RR}{\mathbb{R}}\)</span> <span class="math inline">\(\newcommand{\NN}{\mathbb{N}}\)</span><br />
<span class="math inline">\(\newcommand{\mc}{\mathcal}\)</span><br />
When working on a project with group invariant neural networks, I found myself repeatedly using essentially the same steps to prove many different results on universality of invariant networks. I eventually realized that these steps could be captured in the following commutative diagram:</p>
<figure>
<img src="commutative_diagram.png" width=75%>
</figure>
<p>What’s more: this commutative diagram helps simplify and unify previous proofs of results in the study of invariant neural networks. Plus, it gives a blueprint for developing novel invariant neural network architectures.</p>
<p>In this post, I will explain what this all means. This should be of interest to people who like invariant neural networks, or are curious about applications of some basic topology, algebra, and geometry to the study of neural networks.</p>
<h2 id="definitions">Definitions<a href="#definitions" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h2>
<p>First, an invariant neural network is a neural network <span class="math inline">\(f\)</span> such that certain transformations of its inputs do not affect its outputs. We denote the set of transformations as <span class="math inline">\(G\)</span>, and assume that <span class="math inline">\(G\)</span> is what is known as a <a href="https://en.wikipedia.org/wiki/Group_(mathematics)">group</a> in algebra. Then a neural network <span class="math inline">\(f\)</span> that is <span class="math inline">\(G\)</span>-invariant satisfies <span class="math inline">\(f(gx) = f(x)\)</span> for any <span class="math inline">\(g \in G\)</span>.</p>
<figure>
<img src="duck_translation_inv.png" width=60%>
<figcaption style="font-size:80%">
A neural network should classify both images as having a duck. (Duck photo by <a href="https://unsplash.com/@lozt?utm_source=unsplash&utm_medium=referral&utm_content=creditcopytext">Hoover Tung</a> on <a href="https://unsplash.com/s/photos/duck?utm_source=unsplash&utm_medium=referral&utm_content=creditcopytext">Unsplash</a>)
</figcaption>
</figure>
<p>For instance, if <span class="math inline">\(f\)</span> is an image classifier, then certain translations of the input images should not change the output of <span class="math inline">\(f\)</span>; a picture <span class="math inline">\(x\)</span> with a duck in the bottom right corner and a picture <span class="math inline">\(gx\)</span> with the same duck in the top left corner should both be classified as having a duck. Thus, <span class="math inline">\(f\)</span> should approximately be invariant to the group <span class="math inline">\(G\)</span> of translations. Convolutional neural networks for image classification are approximately invariant to translations.</p>
<p>We say that a network is <em>universal</em> if it can compute all continuous functions that we care about. In particular, we say that a <span class="math inline">\(G\)</span>-invariant network architecture is universal if it can express any continuous <span class="math inline">\(G\)</span>-invariant function.</p>
<p>In the commutative diagram above, <span class="math inline">\(\mathcal X\)</span> is the domain that our input data lies on. We will assume <span class="math inline">\(\mc X \subseteq \RR^D\)</span> is embedded in some Euclidean space of dimension <span class="math inline">\(D\)</span>. The <em>quotient space</em> <span class="math inline">\(\mathcal X / G\)</span> is the set of all equivalence classes of inputs under the transformations of <span class="math inline">\(G\)</span>. In other words, it is the space <span class="math inline">\(\mathcal X\)</span>, but where <span class="math inline">\(gx\)</span> is viewed as equal to <span class="math inline">\(x\)</span> for any <span class="math inline">\(g \in G\)</span>. The <em>quotient map</em> <span class="math inline">\(\pi: \mathcal{X} \to \mathcal{X}/G\)</span> is the map that sends an <span class="math inline">\(x \in \mathcal{X}\)</span> to the equivalence class of all inputs that it can be transformed to, <span class="math inline">\(\pi(x) = \{ gx : g \in G\}\)</span>. Thus, <span class="math inline">\(\pi\)</span> is <span class="math inline">\(G\)</span>-invariant.</p>
<details>
<summary>
Mathematical notes
</summary>
<p>
<p>We will generally assume <span class="math inline">\(G\)</span> is a finite or compact matrix Lie group. A lot can be said when <span class="math inline">\(\mathcal X\)</span> is just a topological space, though we will assume <span class="math inline">\(\mc X\)</span> is embedded in some Euclidean space so that we can process our data with standard neural networks. A more detailed discussion of assumptions can be found in the appendix of our paper [Lim et al. 22], but the topological assumptions are generally very mild.</p>
</p>
</details>
<h2 id="commutative-diagram">Commutative Diagram<a href="#commutative-diagram" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h2>
<p>A <a href="https://en.wikipedia.org/wiki/Commutative_diagram">commutative diagram</a> is a diagram where you can take any path through arrows and arrive at the same result. Let <span class="math inline">\(f: \mathcal{X} \to \mathbb{R}^s\)</span> be any <span class="math inline">\(G\)</span>-invariant continuous function. As an example, the above commutative diagram says that there is an <span class="math inline">\(\tilde f\)</span> such that <span class="math inline">\(f = \tilde f \circ \pi\)</span>, and also an invertible <span class="math inline">\(\psi\)</span> such that <span class="math inline">\(\tilde f \circ \psi^{-1} \circ \psi \circ \pi = f\)</span>.</p>
<figure>
<img src="quotient_cd.png" width=30%>
</figure>
<p>First, consider the subdiagram shown here. This captures the so-called universal property of quotient spaces; for any continuous <span class="math inline">\(G\)</span>-invariant function <span class="math inline">\(f: \mc X \to \RR^s\)</span>, there exists a unique continuous function <span class="math inline">\(\tilde f: \mc X / G \to \RR^s\)</span> on the quotient space such that <span class="math inline">\(f = \tilde f \circ \pi\)</span>. However, this does directly not help us to parameterize <span class="math inline">\(G\)</span>-invariant functions <span class="math inline">\(f\)</span>, as the quotient space <span class="math inline">\(\mc X / G\)</span> that the <span class="math inline">\(\tilde f\)</span> are defined on may look rather non-Euclidean (recall that the quotient space consists of equivalence classes of data points), so we may not know how to parameterize functions on such a domain.</p>
<p>Thus, we will want a topological embedding <span class="math inline">\(\psi : \mc X / G \to \mc Z \subseteq \RR^a\)</span> of the quotient space into some Euclidean space <span class="math inline">\(\RR^a\)</span>, which means <span class="math inline">\(\psi\)</span> is a continuous bijection with continuous inverse <span class="math inline">\(\psi^{-1}\)</span>. This brings us closer to parameterizing <span class="math inline">\(G\)</span>-invariant neural networks. The two functions denoted with dashed red arrows in the commutative diagram are parameterized by neural networks: <span class="math display">\[
\begin{align*}
\phi &amp; = \psi \circ \pi : \mc X \subseteq \RR^D \to \mc Z \subseteq \RR^a \\
\rho &amp; = \tilde f \circ \psi^{-1}: \mc Z \subseteq \RR^a \to \RR^s.
\end{align*}
\]</span> Of course, <span class="math inline">\(f = \tilde f \circ \pi = \tilde f \circ \psi^{-1} \circ \psi \circ \pi = \rho \circ \phi\)</span>. Note that <span class="math inline">\(\rho\)</span> and <span class="math inline">\(\phi\)</span> are compositions of continuous functions, so they are both continuous. Also, their domains are Euclidean spaces, so we can approximate them with standard neural network operations. <span class="math inline">\(\rho\)</span> can be parameterized by an unconstrained neural network. However, <span class="math inline">\(\phi\)</span> is required to be <span class="math inline">\(G\)</span>-invariant, so it seems that we have not made progress, as our original goal was to parameterize <span class="math inline">\(G\)</span>-invariant functions by neural networks. The next section shows how we can utilize structure in <span class="math inline">\(\phi = \psi \circ \pi\)</span> to parameterize <span class="math inline">\(G\)</span>-invariant neural nets.</p>
<h2 id="finding-a-quotient-space-embedding-with-structure">Finding a quotient space embedding with structure<a href="#finding-a-quotient-space-embedding-with-structure" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h2>
<p>To parameterize the <span class="math inline">\(G\)</span>-invariant <span class="math inline">\(\phi = \psi \circ \pi\)</span>, we need to find some structure in <span class="math inline">\(\psi \circ \pi\)</span> that handles the <span class="math inline">\(G\)</span>-invariance. For instance, for processing sets we want permutation invariance, and we can show that in this case for input <span class="math inline">\(x \in \RR^n\)</span> (which represents a set of <span class="math inline">\(n\)</span> real numbers) we may write <span class="math inline">\(\phi(x) = \sum_{i=1}^n \varphi(x_{i})\)</span> for some suitable <span class="math inline">\(\varphi\)</span> that does not have the <span class="math inline">\(G\)</span>-invariance constraint. Thus, we may then parameterize <span class="math inline">\(\varphi\)</span> with a standard neural network. There are different ways to find such structure in <span class="math inline">\(\phi\)</span> or the quotient space embedding.</p>
<p>One way to find structure in <span class="math inline">\(\phi\)</span> is to find structure in a generating set of <span class="math inline">\(G\)</span>-invariant polynomials. A generating set <span class="math inline">\(p_{1}, \ldots, p_{l}\)</span> of the <span class="math inline">\(G\)</span>-invariant polynomials is a set of <span class="math inline">\(G\)</span>-invariant polynomials such that every other <span class="math inline">\(G\)</span>-invariant polynomial <span class="math inline">\(p\)</span> can be written as a polynomial in <span class="math inline">\(p_{1}, \ldots, p_{l}\)</span>. Lemma 11.13 of [González and de Salas 03] shows that the map <span class="math inline">\(h: \mc X \to \RR^l\)</span> given by <span class="math inline">\(h(x) = (p_{1}(x), \ldots, p_{l}(x))\)</span> induces a topological embedding <span class="math inline">\(\tilde h: \mc X / G \to h(\mc X)\)</span> such that <span class="math inline">\(h = \tilde h \circ \pi\)</span>. Thus, choosing <span class="math inline">\(\psi = \tilde h\)</span> as our topological embedding above, we have that <span class="math inline">\(\phi(x) = h(x)\)</span>, so <span class="math inline">\(\phi\)</span> contains all the structure that <span class="math inline">\(h\)</span> does. This is why for the permutation invariant case, we can choose <span class="math inline">\(\phi(x) = \sum_{i=1}^n \varphi(x_{i})\)</span>; as the polynomials of the form <span class="math inline">\(p_{k}(x) = \sum_{i=1}^n x_{i}^k\)</span> for <span class="math inline">\(k=1, \ldots, n\)</span> are generating polynomials of the permutation invariant polynomials, we can write <span class="math display">\[\phi(x) = h(x) = (p_{1}(x), \ldots, p_{n}(x)) = \sum_{i=1}^n (x_{i}, \ldots, x_{i}^n),\]</span> so letting <span class="math inline">\(\varphi(x_{i}) = (x_{i}, \ldots, x_{i}^n)\)</span>, we have a nice sum decomposition structure on <span class="math inline">\(\phi\)</span>.</p>
<p>While this use of the <span class="math inline">\(G\)</span>-invariant polynomials is theoretically general, it can be difficult to use in practice besides for particular choices of <span class="math inline">\(G\)</span>. This is because generators of <span class="math inline">\(G\)</span>-invariant polynomials can be difficult to find for different <span class="math inline">\(G\)</span>, and they can take complicated forms with high degree polynomials (of up to degree <span class="math inline">\(|G|\)</span> by Noether’s bound <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>). Thankfully, efficiently-computable generators are well known for two of the cases we consider below: when the groups are for permutation symmetries or for rotation/reflection symmetries.</p>
<h2 id="parameterizing-with-neural-networks">Parameterizing with neural networks<a href="#parameterizing-with-neural-networks" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h2>
<p>Suppose that the data domain <span class="math inline">\(\mc X\)</span> is <a href="https://en.wikipedia.org/wiki/Compact_space">compact</a>, and that we have some structure such that <span class="math inline">\(\phi\)</span> can be universally approximated by neural networks. Then <span class="math inline">\(\rho\)</span> and <span class="math inline">\(\phi\)</span> can be universally approximated by a neural network.</p>
<p>This is due to an application of standard <a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem">universal approximation results</a>, which say that neural networks can universally approximate continuous functions with domains that are compact subsets of some Euclidean space. If <span class="math inline">\(\mc X\)</span> is compact, then one can show that <span class="math inline">\(\mc X / G\)</span> is compact and thus <span class="math inline">\(\mc Z\)</span> is compact. Hence, <span class="math inline">\(\phi: \mc X \to \mc Z\)</span> and <span class="math inline">\(\rho: \mc Z \to \RR^s\)</span> are continuous functions on compact subsets of Euclidean spaces, so they are amenable to approximation by neural networks.</p>
<h2 id="application-1-deep-sets">Application 1: Deep Sets<a href="#application-1-deep-sets" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h2>
<p>We now give more details on the application of this commutative diagram to neural networks on sets. If we represent a set of <span class="math inline">\(n\)</span> elements as a vector <span class="math inline">\(x \in \mathbb{R}^n\)</span>, then we want any function on the set to not depend on the order of the elements. Thus, letting <span class="math inline">\(G = \mathbb{S}_n\)</span> be the group of permutations on <span class="math inline">\(n\)</span> elements, we want a function <span class="math inline">\(f\)</span> on sets to be invariant to permutations <span class="math inline">\(f(g x) = f(x)\)</span>, where <span class="math inline">\(gx = [x_{g(1)}, \ldots, x_{g(n)}]\)</span> permutes the entries of <span class="math inline">\(x\)</span> by the permutation <span class="math inline">\(g\)</span>.</p>
<p>The invariant DeepSets [Zaheer et al. 17] architecture parameterizes permutation invariant functions as <span class="math display">\[ f(x) \approx \rho_\theta\left(\sum_{i=1}^n \phi_\theta(x_{i}) \right),\]</span> where <span class="math inline">\(\rho_\theta\)</span> and <span class="math inline">\(\phi_\theta\)</span> are neural networks. Their proof that the architecture is universal uses a few computations and facts about symmetric polynomials (see their Appendix A.2).</p>
<p>We can instead prove universality using the above commutative diagram as follows. We take our generating set of <span class="math inline">\(G\)</span>-invariant polynomials as the <a href="https://en.wikipedia.org/wiki/Power_sum_symmetric_polynomial">power sum symmetric polynomials</a>: <span class="math inline">\(p_{k}(x) = \sum_{i=1}^n x_{i}^k\)</span> for <span class="math inline">\(k=1, \ldots, n\)</span>. Thus, we know that in the above commutative diagram, we can write <span class="math display">\[\phi = \psi \circ \pi(x) = \begin{bmatrix}\sum_{i=1}^n x_{i} &amp; \ldots &amp; \sum_{i=1}^n x_{i}^n\end{bmatrix} = \sum_{i=1}^n \begin{bmatrix}x_{i} &amp; \ldots &amp; x_{i}^n\end{bmatrix}.\]</span></p>
<p>Hence, in the DeepSets neural architectures, we can approximate <span class="math inline">\(\rho_{\theta} \approx \rho\)</span>, and <span class="math inline">\(\phi_\theta(y) \approx \begin{bmatrix} y &amp; \ldots &amp; y^n \end{bmatrix}\)</span>, which standard feedforward networks can do, so DeepSets is universal.</p>
<p>We note that this connection to the power sum symmetric polynomials as an embedding of the quotient space is also used in [Finkelshtein et al. 22], see their Appendix C.</p>
<h2 id="application-2-rotation-and-reflection-invariance">Application 2: Rotation and Reflection Invariance<a href="#application-2-rotation-and-reflection-invariance" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h2>
<p>When dealing with certain types of point clouds of points in <span class="math inline">\(\mathbb{R}^d\)</span>, we may want to be invariant to rotations and reflections, which form the group <span class="math inline">\(G = O(d)\)</span> of orthogonal <span class="math inline">\(d \times d\)</span> matrices. [Villar et al. 21] proposed to use some classical results in invariant theory to parameterize <span class="math inline">\(O(d)\)</span>-invariant neural networks. For learning an <span class="math inline">\(O(d)\)</span> invariant function <span class="math inline">\(f: \RR^{n \times d} \to \RR^s\)</span> on <span class="math inline">\(n\)</span> points that satisfies <span class="math inline">\(f(Q v_{1}, \ldots, Qv_{n}) = f(v_{1}, \ldots, v_{n})\)</span> for any <span class="math inline">\(Q \in O(d)\)</span>, they parameterize <span class="math display">\[ f(v_{1}, \ldots, v_{n}) \approx g_{\theta}(V^\top V) = g_{\theta}((v_{i}^\top v_{j})_{i,j=1, \ldots, n}). \]</span> In other words, <span class="math inline">\(g_\theta\)</span> is a neural network acting on all inner products of two points <span class="math inline">\(v_{i}\)</span> and <span class="math inline">\(v_{j}\)</span>. [Villar et al. 21] uses an argument based on the Cholesky decomposition to say that allowing <span class="math inline">\(g_\theta\)</span> to be any arbitrary function gives universality.</p>
<p>To see that we only need continuous <span class="math inline">\(g_\theta\)</span> and that we can thus use a neural network as <span class="math inline">\(g_\theta\)</span>, we note that the First Fundamental Theorem of <span class="math inline">\(O(d)\)</span> from invariant theory says that the inner product polynomials <span class="math inline">\(p_{ij}(v_{1}, \ldots, v_{n}) = v_{i}^\top v_{j}\)</span> are generators of the <span class="math inline">\(O(d)\)</span>-invariant polynomials. Thus, in the above commutative diagram, we can write <span class="math display">\[\phi(x) = \psi \circ \pi(x) = \begin{bmatrix} v_{i}^\top v_{j} \end{bmatrix}_{i,j=1, \ldots, n}.\]</span> So the functions of the form <span class="math inline">\(\rho(\begin{bmatrix} v_{i}^\top v_{j} \end{bmatrix}_{i,j=1, \ldots, n})\)</span> are universal. As <span class="math inline">\(\rho\)</span> is unconstrained and continuous, we can approximate it by a neural network <span class="math inline">\(\rho_\theta \approx \rho\)</span>, so the networks of the form <span class="math inline">\(\rho_\theta( \begin{bmatrix} v_{i}^\top v_{j} \end{bmatrix}_{i,j=1, \ldots, n})\)</span> are universal.</p>
<h2 id="application-3-direct-products-and-eigenvectors">Application 3: Direct Products and Eigenvectors<a href="#application-3-direct-products-and-eigenvectors" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h2>
<p>Our recent paper [Lim et al. 22] inspired this blog post, as I discovered this common pattern underlying the commutative diagram through working on this project. In the paper, we develop universal architectures for a large class of invariances coming from direct products. If <span class="math inline">\(G_i\)</span> is a group acting on <span class="math inline">\(\mc X_i\)</span> for <span class="math inline">\(i=1, \ldots, k\)</span>, then the direct product <span class="math inline">\(G_1 \times \ldots \times G_k\)</span> is the group of tuples <span class="math inline">\((g_1, \ldots, g_k)\)</span> which acts on <span class="math inline">\(\mc X_1 \times \ldots \times \mc X_k\)</span> as <span class="math inline">\((g_1, \ldots, g_k) (x_1, \ldots, x_k) = (g_1 x_1, \ldots, g_k x_k)\)</span>. In our Theorem 1 (the decomposition theorem), we show that a continuous function <span class="math inline">\(f(x_1, \ldots, x_k)\)</span> that is invariant to the large group <span class="math inline">\(G = G_1 \times \ldots \times G_k\)</span> can be parameterized as <span class="math display">\[f(x_1, \ldots, x_k) \approx \rho_\theta\left(\phi_{1,\theta}(x_{1}), \ldots, \phi_{k,\theta}(x_{k}) \right),\]</span> where <span class="math inline">\(\phi_{i,\theta}\)</span> only needs to be invariant to the constituent smaller group <span class="math inline">\(G_{i}\)</span>, and <span class="math inline">\(\rho\)</span> is unconstrained. Further, we can take <span class="math inline">\(\phi_{i,\theta} = \phi_{j,\theta}\)</span> if <span class="math inline">\(\mc X_{i} = \mc X_{j}\)</span> and <span class="math inline">\(G_{i} = G_{j}\)</span>. We prove this theorem precisely through the same-old commutative diagram, where we show that the <span class="math inline">\(\phi\)</span> map has a product structure <span class="math display">\[\phi(x_{1}, \ldots, x_{k}) = (\phi_{1}(x_{1}), \ldots, \phi_{k}(x_{k})),\]</span> for some <span class="math inline">\(G_{i}\)</span>-invariant functions <span class="math inline">\(\phi_{i}\)</span> on the <span class="math inline">\(\mc X_{i}\)</span>.</p>
<p>We proved this result to apply it to a specific invariance arising from learning functions on eigenvectors. For a matrix <span class="math inline">\(A \in \RR^{n \times n}\)</span> with an eigenvector <span class="math inline">\(v \in \RR^n\)</span> of eigenvalue <span class="math inline">\(\lambda\)</span>, note that <span class="math inline">\(-v\)</span> is also an eigenvector of <span class="math inline">\(A\)</span> with the same eigenvalue. A numerical eigenvector algorithm could have returned either <span class="math inline">\(v\)</span> or <span class="math inline">\(-v\)</span>, so we want a function <span class="math inline">\(f\)</span> on eigenvectors <span class="math inline">\(v_{1}, \ldots, v_{k}\)</span> to be invariant to sign flips: <span class="math display">\[f(\pm v_{1}, \ldots, \pm v_{k}) = f(v_{1}, \ldots, v_{k}).\]</span> There are in fact more invariances desired if there is a higher dimensional eigenspace, but we will not cover those here (see [Lim et al. 22] for more on this). This sign invariance is precisely invariance to the action of the group <span class="math inline">\(G = \{1, -1\}^k = \{1, -1\} \times \ldots \times \{1, -1\}\)</span>. Thus, our theorem says that as long as we can parameterize a universal network that is <span class="math inline">\(\{1, -1\}\)</span> invariant, then we can use that to make a <span class="math inline">\(\{1, -1\}^k\)</span> invariant function. Invariance to <span class="math inline">\(\{1, -1\}\)</span> just means that <span class="math inline">\(h(-x) = h(x)\)</span>, so such a function is an even function, which can be simply parameterized as <span class="math inline">\(h(x) = \varphi(x) + \varphi(-x)\)</span> for some unconstrained <span class="math inline">\(\varphi\)</span>. Thus, we can parameterize sign invariant functions on <span class="math inline">\(k\)</span> vectors as <span class="math display">\[f(v_{1}, \ldots, v_{k}) \approx \rho_\theta\left( \phi_\theta(v_{1}) + \phi_\theta(-v_{1}), \  \ldots, \  \phi_{\theta}(v_{k}) + \phi_{\theta}(-v_{k})\right).\]</span> Note that we can use the same <span class="math inline">\(\phi_\theta\)</span> for each vector <span class="math inline">\(v_{i}\)</span> by our decomposition theorem, as <span class="math inline">\(\mc X_{i} = \mc X_{j}\)</span> and <span class="math inline">\(G_{i} = G_{j}\)</span> for each <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>.</p>
<h2 id="limitations">Limitations<a href="#limitations" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h2>
<p>While I think the strategy based on this commutative diagram may be useful for developing new invariant architectures (especially those that come from direct products of groups), this is far from solving every problem in invariant machine learning. For one, many invariant networks are built from equivariant layers followed by a final invariant pooling. Using equivariance has been shown to significantly improve performance in certain applications (e.g. note the success of networks with convolutional layers that are approximately translation equivariant, or see Figure 7 of [Batzner et al. 22] for an example in molecular dynamics). Also, this is mostly concerned with developing universal architectures, but universality is not sufficient for good performance. Inductive biases can greatly improve the performance of invariant architectures, as can be seen by the success of adding attention modules to Set architectures [Lee et al. 19] [Kim et al. 21] or as we saw by adding message passing to our SignNet architecture for eigenvectors of graph operators [Lim et al. 22]. Even if we do desire universality, this commutative diagram approach may not be computationally useful, as there may be no nice quotient space embedding with structure that we can leverage.</p>
<h2 id="references">References<a href="#references" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h2>
<p>[Zaheer et al. 17] Deep Sets.<br />
[Villar et al. 21] Scalars are universal: Equivariant machine learning, structured like classical physics.<br />
[Lim et al. 22] Sign and Basis Invariant Networks for Spectral Graph Representation Learning.<br />
[Finkelshtein et al. 22] A Simple and Universal Rotation Equivariant Point-cloud Network.<br />
[Batzner et al. 21] E(3)-Equivariant Graph Neural Networks for Data-Efficient and Accurate Interatomic Potentials.<br />
[Lee et al. 19] Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks.<br />
[González and de Salas 03] C∞-differentiable spaces. Springer.<br />
[Kim et al. 21] Transformers Generalize DeepSets and Can be Extended to Graphs and Hypergraphs.<br />
[Kraft and Procesi 96]. Classical Invariant Theory.</p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>Noether’s bound says that the <span class="math inline">\(G\)</span>-invariant polynomials of degree at most <span class="math inline">\(|G|\)</span> generate all <span class="math inline">\(G\)</span>-invariant polynomials. See e.g. [Kraft and Procesi 96] Theorem 2.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

			</div>
			<hr class="post-end">
			<footer class="post-info">
				<p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>2723 Words</p>
				<p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg>2022-03-05 16:00 -0800</p>
			</footer>
		</article>
		<div class="post-nav thin">
			<a class="prev-post" href="https://cptq.github.io/posts/flags/">
				<span class="post-nav-label">Older&nbsp;<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-right"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg></span><br><span>Flags Ranked by Matrix Rank</span>
			</a>
		</div>
		<div id="comments" class="thin">
</div>
	</main>

	<footer id="site-footer" class="section-inner thin animated fadeIn faster">
		<p>&copy; 2024 <a href="https://cptq.github.io">Derek Lim</a></p>
		<p>
			Made with <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> &#183; Theme <a href="https://github.com/Track3/hermit" target="_blank" rel="noopener">Hermit</a>
			
		</p>
	</footer>



	<script src="https://cptq.github.io/js/bundle.min.7d8545daa55d62427355498dd8da13f98ff79a7938ce7d2a5e2ae1ec0de3beb8.js" integrity="sha256-fYVF2qVdYkJzVUmN2NoT+Y/3mnk4zn0qXirh7A3jvrg=" crossorigin="anonymous"></script>
	

</body>

</html>
